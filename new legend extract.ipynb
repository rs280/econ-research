{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170a905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rishi/.zshenv:.:2: no such file or directory: /Users/rishi/.cargo/env\n",
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/27/4b/7c1a00c2c3fbd004253937f7520f692a9650767aa73894d7a34f0d65d3f4/openai-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai) (4.8.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Obtaining dependency information for jiter<1,>=0.10.0 from https://files.pythonhosted.org/packages/10/c1/40c9f7c22f5e6ff715f28113ebaba27ab85f9af2660ad6e1dd6425d14c19/jiter-0.12.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading jiter-0.12.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai) (2.10.4)\n",
      "Requirement already satisfied: sniffio in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/rishi/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Downloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.12.0-cp311-cp311-macosx_11_0_arm64.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m320.5/320.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jiter, distro, openai\n",
      "Successfully installed distro-1.9.0 jiter-0.12.0 openai-2.14.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Configured OPENAI provider\n"
     ]
    }
   ],
   "source": [
    "%pip install openai\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import base64\n",
    "import yaml\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Optional, Any, Union\n",
    "%pip install openai\n",
    "from openai import OpenAI\n",
    "API_KEY_PATH = Path(\"apikey.yaml\")\n",
    "\n",
    "def _load_api_config() -> Dict[str, Any]:\n",
    "    if not API_KEY_PATH.exists():\n",
    "        return {}\n",
    "    with API_KEY_PATH.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        data = yaml.safe_load(handle) or {}\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(\"apikey.yaml must contain a mapping at the root\")\n",
    "        return data\n",
    "\n",
    "API_CONFIG = _load_api_config()\n",
    "\n",
    "def _get_provider_key(provider_name: str, env_var: str) -> Optional[str]:\n",
    "    provider_cfg = (API_CONFIG.get(\"providers\") or {}).get(provider_name, {})\n",
    "    if isinstance(provider_cfg, dict):\n",
    "        key = provider_cfg.get(\"api_key\")\n",
    "        if key:\n",
    "            return key\n",
    "    return os.getenv(env_var)\n",
    "\n",
    "LLM_PROVIDER = (os.getenv(\"LLM_PROVIDER\") or API_CONFIG.get(\"default_provider\") or \"gemini\").lower() \n",
    "\n",
    "GEMINI_MODEL_NAME = API_CONFIG.get(\"models\", {}).get(\"gemini\", \"gemini-1.5-flash\")\n",
    "OPENAI_MODEL_NAME = API_CONFIG.get(\"models\", {}).get(\"openai\", \"gpt-4o\")\n",
    "CLAUDE_MODEL_NAME = API_CONFIG.get(\"models\", {}).get(\"anthropic\", \"claude-3-5-sonnet-20240620\")\n",
    "\n",
    "gemini_model = None\n",
    "openai_client = None\n",
    "anthropic_client = None\n",
    "\n",
    "if LLM_PROVIDER == \"gemini\":\n",
    "    import google.generativeai as genai\n",
    "    from google.generativeai import GenerationConfig\n",
    "    gemini_key = _get_provider_key(\"gemini\", \"GOOGLE_API_KEY\")\n",
    "    if not gemini_key:\n",
    "        raise RuntimeError(\"Gemini API key missing. Provide via apikey.yaml or GOOGLE_API_KEY env var.\")\n",
    "    genai.configure(api_key=gemini_key)\n",
    "    gemini_model = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
    "\n",
    "elif LLM_PROVIDER == \"openai\":\n",
    "    \n",
    "    openai_key = _get_provider_key(\"openai\", \"OPENAI_API_KEY\")\n",
    "    if not openai_key:\n",
    "        raise RuntimeError(\"OpenAI API key missing. Provide via apikey.yaml or OPENAI_API_KEY env var.\")\n",
    "    openai_client = OpenAI(api_key=openai_key)\n",
    "\n",
    "elif LLM_PROVIDER == \"anthropic\":\n",
    "    import anthropic\n",
    "    anthropic_key = _get_provider_key(\"anthropic\", \"ANTHROPIC_API_KEY\")\n",
    "    if not anthropic_key:\n",
    "        raise RuntimeError(\"Anthropic API key missing. Provide via apikey.yaml or ANTHROPIC_API_KEY env var.\")\n",
    "    anthropic_client = anthropic.Anthropic(api_key=anthropic_key)\n",
    "\n",
    "print(f\"‚úÖ Configured {LLM_PROVIDER.upper()} provider\")\n",
    "\n",
    "\n",
    "def _image_to_png_base64(image_data: np.ndarray) -> Tuple[str, str]:\n",
    "    \"\"\"Return (media_type, base64_data) for the image payload.\"\"\"\n",
    "    if image_data.dtype != np.uint8:\n",
    "        image_data = (image_data * 255).astype(np.uint8)\n",
    "        \n",
    "    img_pil = Image.fromarray(image_data)\n",
    "    if img_pil.mode != 'RGB':\n",
    "        img_pil = img_pil.convert('RGB')\n",
    "        \n",
    "    buf = io.BytesIO()\n",
    "    img_pil.save(buf, format=\"PNG\")\n",
    "    b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "    return \"image/png\", b64\n",
    "\n",
    "\n",
    "def _extract_json_from_text(text: str) -> str:\n",
    "    \"\"\"Best-effort extraction of a JSON object from a text response.\"\"\"\n",
    "    if not text:\n",
    "        raise ValueError(\"Empty response from LLM\")\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    if text.startswith(\"```json\"):\n",
    "        text = text[7:]\n",
    "    if text.startswith(\"```\"):\n",
    "        text = text[3:]\n",
    "    if text.endswith(\"```\"):\n",
    "        text = text[:-3]\n",
    "        \n",
    "    text = text.strip()\n",
    "    \n",
    "    if text.startswith(\"{\") and text.endswith(\"}\"):\n",
    "        return text\n",
    "        \n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        raise ValueError(f\"No JSON object found in response: {text[:200]}\")\n",
    "        \n",
    "    return text[start:end+1]\n",
    "\n",
    "\n",
    "def _legend_prompt() -> str:\n",
    "    return \"\"\"You are an expert GIS analyst extracting the LEGEND/KEY from a historical zoning map image. \n",
    "Return STRICT JSON only. No preamble.\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. **Detection**: Identify ANY legend, key, reference box, or table that explains patterns, letters, or zone codes.\n",
    "2. **Bounding Box**: Provide integer pixel coordinates relative to the image size provided.\n",
    "3. **Content**: Extract every item (symbol, color, or text code) and its meaning.\n",
    "\n",
    "Output JSON schema:\n",
    "{\n",
    "  \"has_legend\": boolean,\n",
    "  \"legend_bbox\": {\"x\": int, \"y\": int, \"width\": int, \"height\": int},\n",
    "  \"legend_location\": \"top-left\" | \"top-right\" | \"bottom-left\" | \"bottom-right\" | \"left\" | \"right\" | \"center\",\n",
    "  \"legend_items\": [\n",
    "    {\n",
    "      \"identifier\": string,\n",
    "      \"symbol_pattern_type\": string,\n",
    "      \"visual_description\": string,\n",
    "      \"meaning\": string\n",
    "    }\n",
    "  ],\n",
    "  \"map_metadata\": {\n",
    "      \"map_title\": string,\n",
    "      \"map_location\": string,\n",
    "      \"map_year\": string,\n",
    "      \"confidence_level\": \"high\" | \"medium\" | \"low\"\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "def extract_legend_with_llm(image_path: Union[str, Path], image_data: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"Extract legend information using the configured provider.\"\"\"\n",
    "    path_obj = Path(image_path)\n",
    "    \n",
    "    def _infer_bbox_from_location(img: np.ndarray, loc: Optional[str]) -> Dict[str, int]:\n",
    "        h, w = img.shape[:2]\n",
    "        box_w, box_h = int(0.30 * w), int(0.30 * h)\n",
    "        loc = (loc or \"\").lower()\n",
    "        \n",
    "        x, y = max(0, w - box_w - int(0.05 * w)), max(0, h - box_h - int(0.05 * h))\n",
    "\n",
    "        if \"top\" in loc:\n",
    "            y = int(0.05 * h)\n",
    "        if \"bottom\" in loc:\n",
    "            y = max(0, h - box_h - int(0.05 * h))\n",
    "        if \"left\" in loc:\n",
    "            x = int(0.05 * w)\n",
    "        if \"right\" in loc:\n",
    "            x = max(0, w - box_w - int(0.05 * w))\n",
    "        if \"center\" in loc:\n",
    "            x, y = int(w/2 - box_w/2), int(h/2 - box_h/2)\n",
    "            \n",
    "        return {\n",
    "            \"x\": int(np.clip(x, 0, w - 1)),\n",
    "            \"y\": int(np.clip(y, 0, h - 1)),\n",
    "            \"width\": int(min(box_w, w)),\n",
    "            \"height\": int(min(box_h, h)),\n",
    "        }\n",
    "\n",
    "    def _normalize_bbox_local(raw_bbox: Any) -> Optional[Dict[str, int]]:\n",
    "        if not raw_bbox:\n",
    "            return None\n",
    "            \n",
    "        if isinstance(raw_bbox, dict):\n",
    "            if {'x', 'y', 'width', 'height'}.issubset(raw_bbox.keys()):\n",
    "                return {k: int(raw_bbox[k]) for k in ['x', 'y', 'width', 'height']}\n",
    "            if {'x1', 'y1', 'x2', 'y2'}.issubset(raw_bbox.keys()):\n",
    "                x1, y1, x2, y2 = [int(raw_bbox[k]) for k in ['x1', 'y1', 'x2', 'y2']]\n",
    "                return {\"x\": x1, \"y\": y1, \"width\": max(0, x2 - x1), \"height\": max(0, y2 - y1)}\n",
    "                \n",
    "        if isinstance(raw_bbox, (list, tuple)) and len(raw_bbox) == 4:\n",
    "            return {\"x\": int(raw_bbox[0]), \"y\": int(raw_bbox[1]), \"width\": int(raw_bbox[2]), \"height\": int(raw_bbox[3])}\n",
    "            \n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        prompt = _legend_prompt()\n",
    "        raw_text = \"\"\n",
    "\n",
    "        if LLM_PROVIDER == \"gemini\":\n",
    "            if not gemini_model: raise RuntimeError(\"Gemini not configured\")\n",
    "            \n",
    "            img_pil = Image.fromarray(image_data)\n",
    "            resp = gemini_model.generate_content(\n",
    "                [prompt, img_pil],\n",
    "                generation_config=GenerationConfig(\n",
    "                    response_mime_type=\"application/json\",\n",
    "                    temperature=0.2,\n",
    "                ),\n",
    "            )\n",
    "            raw_text = resp.text\n",
    "\n",
    "        elif LLM_PROVIDER == \"openai\":\n",
    "            if not openai_client: raise RuntimeError(\"OpenAI not configured\")\n",
    "            \n",
    "            media_type, b64 = _image_to_png_base64(image_data)\n",
    "            data_url = f\"data:{media_type};base64,{b64}\"\n",
    "            \n",
    "            resp = openai_client.chat.completions.create(\n",
    "                model=OPENAI_MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}},\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "            raw_text = resp.choices[0].message.content\n",
    "\n",
    "        elif LLM_PROVIDER == \"anthropic\":\n",
    "            if not anthropic_client: raise RuntimeError(\"Anthropic not configured\")\n",
    "            \n",
    "            media_type, b64 = _image_to_png_base64(image_data)\n",
    "            resp = anthropic_client.messages.create(\n",
    "                model=CLAUDE_MODEL_NAME,\n",
    "                max_tokens=2000,\n",
    "                temperature=0.2,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\n",
    "                                \"type\": \"image\",\n",
    "                                \"source\": {\"type\": \"base64\", \"media_type\": media_type, \"data\": b64},\n",
    "                            },\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            raw_text = \"\".join([b.text for b in resp.content if getattr(b, \"type\", None) == \"text\"])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported LLM_PROVIDER: {LLM_PROVIDER}\")\n",
    "\n",
    "        clean_json = _extract_json_from_text(raw_text)\n",
    "        legend_data = json.loads(clean_json)\n",
    "        \n",
    "        legend_data['source_file'] = path_obj.name\n",
    "\n",
    "        raw_bbox = (legend_data.get('legend_bbox') or \n",
    "                   legend_data.get('legend_bounding_box') or \n",
    "                   legend_data.get('bbox'))\n",
    "                   \n",
    "        norm_bbox = _normalize_bbox_local(raw_bbox)\n",
    "        \n",
    "        if norm_bbox is None and legend_data.get('has_legend'):\n",
    "            norm_bbox = _infer_bbox_from_location(image_data, legend_data.get('legend_location'))\n",
    "            \n",
    "        legend_data['legend_bbox'] = norm_bbox\n",
    "\n",
    "        return legend_data\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'has_legend': False,\n",
    "            'error': str(e),\n",
    "            'source_file': path_obj.name,\n",
    "            'raw_response': locals().get('raw_text', '')\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd7026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading image: input/appleton-post-crescent-oct-24-1922-p-13.png...\n",
      "ü§ñ Analyzing with None...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/grpc/_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1175\u001b[0m (\n\u001b[1;32m   1176\u001b[0m     state,\n\u001b[1;32m   1177\u001b[0m     call,\n\u001b[1;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1180\u001b[0m )\n\u001b[0;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Getting metadata from plugin failed with error: ('invalid_grant: Bad Request', {'error': 'invalid_grant', 'error_description': 'Bad Request'})\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Getting metadata from plugin failed with error: (\\'invalid_grant: Bad Request\\', {\\'error\\': \\'invalid_grant\\', \\'error_description\\': \\'Bad Request\\'})\", grpc_status:14, created_time:\"2025-12-26T10:23:52.286647+05:30\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m: 503 Getting metadata from plugin failed with error: ('invalid_grant: Bad Request', {'error': 'invalid_grant', 'error_description': 'Bad Request'})",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mü§ñ Analyzing with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLM_PROVIDER\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 4. Run the Extraction\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Note: We pass the path (for logging) and the data (for analysis)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mextract_legend_with_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 5. Print Results\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 188\u001b[0m, in \u001b[0;36mextract_legend_with_llm\u001b[0;34m(image_path, image_data)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# Gemini accepts PIL images directly\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     img_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(image_data)\n\u001b[0;32m--> 188\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mgemini_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_pil\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGenerationConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse_mime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     raw_text \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m LLM_PROVIDER \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:164\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         _retry_error_helper(\n\u001b[1;32m    154\u001b[0m             exc,\n\u001b[1;32m    155\u001b[0m             deadline,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m             timeout,\n\u001b[1;32m    162\u001b[0m         )\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleep generator stopped yielding sleep values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Batch process all maps in input folder\n",
    "input_folder = \"input/\"\n",
    "output_file = \"output/legend_extractions_llm.json\"\n",
    "\n",
    "image_files = glob.glob(os.path.join(input_folder, \"*.png\"))\n",
    "\n",
    "print(f\"üîç Found {len(image_files)} maps to process\\n\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, img_path in enumerate(image_files, 1):\n",
    "    print(f\"[{idx}/{len(image_files)}] Processing: {os.path.basename(img_path)}...\")\n",
    "    \n",
    "    try:\n",
    "        pil_image = Image.open(img_path).convert('RGB')\n",
    "        image_data = np.array(pil_image)\n",
    "        \n",
    "        result = extract_legend_with_llm(img_path, image_data)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        if result.get('has_legend'):\n",
    "            bbox = result.get('legend_bbox', {})\n",
    "            print(f\"  ‚úÖ Legend found at: ({bbox.get('x')}, {bbox.get('y')}, {bbox.get('width')}x{bbox.get('height')})\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå No legend detected\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Error: {e}\")\n",
    "        all_results.append({\n",
    "            'source_file': os.path.basename(img_path),\n",
    "            'has_legend': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Save all results\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Saved {len(all_results)} results to {output_file}\")\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in all_results if r.get('has_legend'))\n",
    "print(f\"\\nüìä Summary: {successful}/{len(all_results)} maps had detectable legends\")\n",
    "\n",
    "# Display sample result\n",
    "if all_results:\n",
    "    print(\"\\n--- Sample Result ---\")\n",
    "    print(json.dumps(all_results[0], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
